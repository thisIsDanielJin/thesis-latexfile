\chapter{Introduction}
\section{Motivation}
The transition from IPv4 to IPv6 has been talked about for years, but it's finally becoming a real operational headache for companies running Kubernetes clusters. During my internship at SAP, I witnessed firsthand how teams struggle with this problem - IPv4 addresses are getting expensive (we're talking about 50 dollars per address in AWS), but you can't just flip a switch to IPv6 when half your enterprise customers still run legacy IPv4-only systems.
What makes this particularly interesting is that Kubernetes adds another layer of complexity. Unlike traditional networks where you might have a few hundred servers, a single Kubernetes cluster can spawn thousands of pods dynamically, each potentially needing its own IP address. The standard approaches - either running both protocols simultaneously (Dual-Stack) or using translation mechanisms (NAT64/DNS64) - each come with their own problems that aren't well-documented in cloud-native environments.
The real kicker is that most existing research treats this as a pure networking problem, ignoring how Kubernetes' orchestration model completely changes the game. When pods are ephemeral and can scale from 10 to 1000 instances in minutes, traditional IPv6 transition strategies start breaking down in ways nobody really talks about in the literature.


\section{Problem Statement}
The core problem is that Kubernetes operators are stuck between a rock and a hard place. They need to adopt IPv6 to avoid spiraling IPv4 costs and meet compliance requirements, but they also need to maintain compatibility with legacy systems that will probably outlive us all. The two main approaches available today force painful trade-offs that nobody has properly quantified in Kubernetes contexts.
Running Dual-Stack means managing two parallel network configurations - double the routing tables, double the firewall rules, and double the chances something goes wrong at 3 AM. I've seen deployments where a simple typo in an IPv6 CIDR range took down half a cluster because the debugging tools everyone knows are still IPv4-centric. On the flip side, NAT64/DNS64 promises a cleaner IPv6-only infrastructure, but it introduces translation overhead that could be catastrophic when you're dealing with latency-sensitive microservices making thousands of API calls per second.
What's worse is that CLAT (Customer-side transLATor) deployments in Kubernetes are basically uncharted territory. Should you run one translator per node? Per pod? As a sidecar container? Nobody knows because nobody has actually tested this at scale. The few blog posts that exist are either vendor marketing or "it worked on my laptop" tutorials that fall apart under real workloads.


\section{Objectives and Scope}
This thesis aims to cut through the confusion by systematically comparing these approaches in real Kubernetes environments. I'm not interested in theoretical models or synthetic benchmarks - I want to know what actually happens when you deploy these solutions in production-like scenarios.
Specifically, I'll evaluate three deployment strategies: pure Dual-Stack as our baseline, NAT64/DNS64 with per-node CLAT, and NAT64/DNS64 with per-container CLAT. For each approach, I'll measure the stuff that actually matters to operators: How much latency does translation add? At what point does the system fall over? How much CPU and memory overhead are we talking about? And perhaps most importantly - what breaks when things go wrong?
The scope is deliberately focused on managed Kubernetes environments, particularly SAP Gardener, because that's where the rubber meets the road. I'm not trying to solve IPv6 transition for the entire internet - just trying to give Kubernetes operators actionable data to make informed decisions. This means testing with realistic workloads (think microservices, not ping floods), actual CNI plugins people use (Calico, Cilium), and failure scenarios that keep SREs up at night.

\section{Cooperation with SAP SE}
This thesis is being conducted in cooperation with SAP SE's Gardener team, who are dealing with these exact challenges across their managed Kubernetes offering. SAP provides not just the infrastructure for testing but also real-world use cases from their enterprise customers who are demanding IPv6 support while simultaneously running 20-year-old ERP systems that barely understand what IPv6 is.
Working with SAP gives me access to production-grade Gardener clusters where I can test at scales impossible in a university lab. We're talking about clusters with hundreds of nodes, thousands of pods, and actual traffic patterns from enterprise workloads. The Gardener team has also been invaluable in identifying edge cases - like what happens when your NAT64 gateway needs to handle SAP's infamous "month-end processing" traffic spikes.
The collaboration ensures that this research isn't just academically interesting but actually useful for operators. SAP has committed to integrating successful findings into Gardener's documentation and potentially its default configurations, which could impact thousands of Kubernetes clusters worldwide. Plus, having access to their engineering team means I can validate my results against their production experiences - if my tests show NAT64 adds 50ms of latency but they're seeing 5ms in production, I know something's wrong with my setup.