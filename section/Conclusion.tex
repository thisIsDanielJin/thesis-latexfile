\chapter{Conclusion}
This chapter revisits the thesis question, whether client-side translation via CLAT imposes a performance penalty compared to Dual-Stack and synthesizes the empirical evidence gathered across three environments and three Linux CLAT implementations. It draws together the main findings on throughput and latency, interprets their significance in light of platform and topology effects and outlines directions for future work that would extend the scope from benchmarks toward system level validation in containerized environments.

\paragraph{Summary of Key Findings}
The measurements show that CLAT can deliver performance close to native Dual-Stack for the tested workloads and that the magnitude of any overhead depends less on the abstract notion of “translation” and more on where the performance bottleneck lies. When the host datapath was the limiting resource, the kernel-space implementation achieved substantially higher TCP throughput and modestly lower RTT than user-space translators, consistent with the expected advantages of avoiding additional copies and context switches and of maintaining cache locality in the kernel datapath. Conversely, when the bottleneck moved to a 1 Gbit/s Ethernet link, the differences among CLAT implementations compressed toward the link limit and the gap to the Dual-Stack baselines reflected the shared network bottleneck rather than translation overhead. This pattern was consistent across environments. In the bare-metal host setup the one-hop and two-hop IPv6 baselines established the cost of an extra namespace traversal and the kernel versus user space gap aligned with that baseline separation. In the bare-metal network setup, overall rates converged and the relative ordering persisted only as a small difference within the link cap.

Timing sources emerged as a primary factor in virtualized and local runs alike. In the AWS environment, kvm-clock produced pronounced swings in both the native IPv6 baseline and Jool despite constant topology and workload, while Tayga and Tundra appeared stable at lower rates. Switching to hpet reduced but did not fully eliminate irregularities in the baseline time series. On bare metal, tsc yielded higher reported loopback throughputs and on the 1 Gbit/s link whereas hpet smoothed traces at the cost of higher timing overhead. These effects underscore that reliable comparisons require interpreting translation results relative to baselines taken under the same clocksource. Furthermore, they also explain why cloud based measurements, if not carefully controlled, can obscure datapath differences behind platform jitter.

RTT results mirror the throughput trends. Across environments, Jool consistently showed a small latency advantage over user-space translation, while absolute RTTs scaled with CPU capabilities and the presence of a physical hop. The ordering among translators remained stable even as absolute values shifted between cloud, bare-metal host and bare-metal network conditions. Together, the throughput and RTT evidence supports the central claim: for the tested single flow TCP and ICMP workloads, CLAT introduces only a limited overhead relative to Dual-Stack and that overhead becomes practically negligible when the network link, not the host, is the binding constraint. This finding strengthens the case for IPv6-first designs that rely on 464XLAT to maintain IPv4 reachability, especially in settings where IPv4 scarcity or cost pressures make Dual-Stack unattractive.

\paragraph{Future Work}

Several extensions would broaden both the technical depth and operational relevance of the conclusions. First, additional metrics should be incorporated to illuminate the costs and limits of translation under load. CPU utilization would quantify efficiency and reveal headroom. Memory bandwidth and cache behavior would connect datapath choices to hardware limits. Tail latency distributions under sustained traffic are particularly important for user experience and could be coupled with congestion control dynamics to capture impacts not visible in averages. 

Second, future experiments should remove or raise the external bottleneck to expose intrinsic translator limits. Running on hosts with 10/25/40/100 Gbit/s network interface cards and exploring fast paths would help distinguish translator overhead from network interface card constraints. These setups would also allow a more direct comparison between Jool’s kernel datapath and user-space translators when neither the link nor the timing source is the dominant limiter.

Third, concurrency should be used to exercise Tundra’s multi-threaded design and to establish scaling curves for user space translation. Using many parallel iperf flows, mixed short and long connections would test whether Tundra approaches kernel-space performance. 

Finally, the motivation for this work, supporting IPv6-only clusters and services, calls for validation in containerized environments. Integrating CLAT into Kubernetes, for example by packaging the CLAT as a DaemonSet with per node configuration, would enable measurements across common CNIs. An automated test environment that provisions clusters, applies network policies, configures translators, and runs reproducible benchmarks would turn the methodology into a continuous integration workflow and provide operational guidance for adopting 464XLAT at scale.
